import os
import time
import streamlit as st
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from docx import Document as DocxLoader
import fitz  # pymupdf

def read_docx(path: str):
    doc = DocxLoader(path)
    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    return [Document(page_content=text, metadata={"source": path.split('/')[-1]})]

def read_pdf(path: str):
    # loader = PyPDFLoader(file_path)
    # documents = loader.load()
    # return documents

    doc = fitz.open(path)
    all_text = ""
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)  # í˜ì´ì§€ ë¡œë“œ
        text = page.get_text()          # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        all_text += f"\n{text}"

    doc.close()
    return [Document(page_content=all_text, metadata={"source": path.split('/')[-1]})]

def get_document(file_path: str):
    if file_path.endswith(".docx"):
        return read_docx(file_path)
    elif file_path.endswith(".pdf"):
        return read_pdf(file_path)
    else:
        print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.{file_path.split('/')[-1]}")

def create_chroma_db(dir_path):
    
    documents = []
    # collection = chromadb.Client().create_collection(name="docs")

    for file_name in os.listdir(dir_path):
        print(f"file_name : {file_name}")
        file_path = os.path.join(dir_path, file_name)
        documents += get_document(file_path)
        

    # 3. ë¬¸ì„œ ë¶„í•  (chunking)
    # text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    text_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
    split_docs = text_splitter.split_documents(documents)


    # 4. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ (OpenAI)
    embedding = OpenAIEmbeddings()

    # 5. Chroma ë²¡í„° DBì— ì €ì¥
    vectordb = Chroma.from_documents(split_docs, embedding, persist_directory="./chroma_db")
    vectordb.persist()

    return vectordb

def load_chroma_db():
    embeddings = OpenAIEmbeddings()
    vectordb = Chroma(
        persist_directory="./chroma_db",
        embedding_function=embeddings
    )
    return vectordb









class Chroma2:

    def create_langchain():
        
        start_time = time.time()

        # print(f"OPENAI_API_KEY={os.environ["OPENAI_API_KEY"]}")
        dir_path = "./Documents"

        # ìµœì´ˆ 1íšŒë§Œ ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
        if "vectordb" not in st.session_state:
            vectordb = load_chroma_db()
            print(len(vectordb))

            if len(vectordb) == 0:
                # ë²¡í„°DB ì´ˆê¸°í™”
                print("âœ… ë²¡í„°DB íŒŒì¼ ìƒì„±")
                vectordb = create_chroma_db(dir_path)
           
            st.session_state.vectordb = vectordb
            print("âœ… ë²¡í„°DB íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°")
        else:
            print("ğŸ“¦ ë²¡í„°DB ì„¸ì…˜ ë¶ˆëŸ¬ì˜¤ê¸°")
            

        vectordb = st.session_state.vectordb
        print(len(vectordb))
        elapsed = time.time() - start_time
        print(f"â±ï¸ ë²¡í„°DB ìˆ˜í–‰ : {elapsed:.4f}ì´ˆ")


        # 6. Retriever ìƒì„±
        retriever = vectordb.as_retriever(search_kwargs={"k": 3})

        custom_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=
            "ë‹¹ì‹ ì€ ì‹ í•œê¸ˆìœµê·¸ë£¹ì˜ ICT ì‚¬ì—…ì •ë³´ë¥¼ ë¶„ì„í•´ì£¼ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤."
            "ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì‹¤ì— ê·¼ê±°í•œ ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ì œê³µí•©ë‹ˆë‹¤."
            "ë¬¸ì„œ:"
            "{context}"
            "ì§ˆë¬¸:"
            "{question}"
        )
        # print(custom_prompt)        

        # 7. QA ì²´ì¸ ìƒì„± (Retriever + ChatGPT)
        qa_chain = RetrievalQA.from_chain_type(
            llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.2),
            # llm=ChatOpenAI(model_name="gpt-4o-mini", temperature=0.2),
            # llm=ChatOpenAI(model_name="gpt-4", temperature=0.2),
            chain_type="stuff", 
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": custom_prompt}
            # chain_type="map_reduce"
            # chain_type_kwargs={"question_prompt": chat_prompt}
        )

        return qa_chain


# 8. ì§ˆì˜
# query = "ì‹ í•œì€í–‰ì˜ ìµœê·¼ ì‹ ì‚¬ì—…ì€?"
# query = "ë² íŠ¸ë‚¨ì€í–‰ ì‚¬ì—… ì…ì°° ìê²©ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜"
# langchain = Chroma2.create_langchain()
# result = langchain(query)

# print("\nğŸ“Œ ë‹µë³€:")
# print(result["result"])

# print("\nğŸ“š ê´€ë ¨ ë¬¸ì„œ:")
# for doc in result["source_documents"]:
#     print(f"- {doc.metadata['source']}")